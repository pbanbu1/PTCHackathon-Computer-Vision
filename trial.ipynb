{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.3\n2.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG16\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import os, shutil\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import seaborn\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tf-nightly\n",
      "  Downloading tf_nightly-2.5.0.dev20210218-cp38-cp38-manylinux2010_x86_64.whl (408.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 408.4 MB 6.8 kB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (3.3.0)\n",
      "Collecting tb-nightly~=2.5.0.a\n",
      "  Downloading tb_nightly-2.5.0a20210218-py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 7.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (0.11.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.1.0)\n",
      "Collecting tf-estimator-nightly~=2.5.0.dev\n",
      "  Downloading tf_estimator_nightly-2.5.0.dev2021021801-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 9.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.19.4)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.1.2)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 3.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (3.14.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.12.1)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 13.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (0.36.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/pbanbu/.local/lib/python3.8/site-packages (from tf-nightly) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/pbanbu/.local/lib/python3.8/site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/pbanbu/.local/lib/python3.8/site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.4.0,>=0.3.0\n",
      "  Downloading tensorboard_data_server-0.3.0-py3-none-manylinux2010_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 3.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/pbanbu/.local/lib/python3.8/site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (45.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/pbanbu/.local/lib/python3.8/site-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/pbanbu/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/pbanbu/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.2.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard-data-server, grpcio, tb-nightly, tf-estimator-nightly, gast, h5py, tf-nightly\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.32.0\n",
      "    Uninstalling grpcio-1.32.0:\n",
      "      Successfully uninstalled grpcio-1.32.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed gast-0.4.0 grpcio-1.34.1 h5py-3.1.0 tb-nightly-2.5.0a20210218 tensorboard-data-server-0.3.0 tf-estimator-nightly-2.5.0.dev2021021801 tf-nightly-2.5.0.dev20210218\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "from keras.applications.vgg19 import VGG19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG19(weights='imagenet',include_top=False,input_shape=(326, 326, 3), classifier_activation = 'softmax')\n",
    "# conv_base = EfficientNetB0(\n",
    "#     include_top=False, weights='imagenet', input_tensor=None,\n",
    "#     input_shape=(350, 350, 3),\n",
    "#     classifier_activation='softmax'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"vgg19\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_10 (InputLayer)        [(None, 326, 326, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 326, 326, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 326, 326, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 163, 163, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 163, 163, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 163, 163, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 81, 81, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 81, 81, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 81, 81, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 81, 81, 256)       590080    \n_________________________________________________________________\nblock3_conv4 (Conv2D)        (None, 81, 81, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 40, 40, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 40, 40, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 40, 40, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 40, 40, 512)       2359808   \n_________________________________________________________________\nblock4_conv4 (Conv2D)        (None, 40, 40, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 20, 20, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 20, 20, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 20, 20, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 20, 20, 512)       2359808   \n_________________________________________________________________\nblock5_conv4 (Conv2D)        (None, 20, 20, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 10, 10, 512)       0         \n=================================================================\nTotal params: 20,024,384\nTrainable params: 20,024,384\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(conv_base, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir_rust1 = 'Image Classification/Stage 1'\n",
    "original_dataset_dir_rust2 = 'Image Classification/Stage 2'\n",
    "original_dataset_dir_rust3 = 'Image Classification/Stage 3'\n",
    "original_dataset_dir_rust4 = 'Image Classification/Stage 4'\n",
    "original_dataset_dir_norust = 'norust'\n",
    "\n",
    "#dir to store images\n",
    "store_directory = 'ttv_images'\n",
    "train_dir = os.path.join(store_directory, 'train')\n",
    "validation_dir = os.path.join(store_directory, 'validation')\n",
    "test_dir = os.path.join(store_directory, 'test')\n",
    "\n",
    "rust1_train = os.path.join(train_dir, 'Rust (Stage 1)')\n",
    "rust2_train = os.path.join(train_dir, 'Rust (Stage 2)')\n",
    "rust3_train = os.path.join(train_dir, 'Rust (Stage 3)')\n",
    "rust4_train = os.path.join(train_dir, 'Rust (Stage 4)')\n",
    "norust_train = os.path.join(train_dir, 'No Rust')\n",
    "\n",
    "norust_validation = os.path.join(validation_dir, 'No Rust ')\n",
    "rust1_validation = os.path.join(validation_dir, 'Rust (Stage 1)')\n",
    "rust2_validation = os.path.join(validation_dir, 'Rust (Stage 2)')\n",
    "rust3_validation = os.path.join(validation_dir, 'Rust (Stage 3)')\n",
    "rust4_validation = os.path.join(validation_dir, 'Rust (Stage 4)')\n",
    "\n",
    "rust1_test = os.path.join(test_dir, 'Rust (Stage 1)')\n",
    "rust2_test = os.path.join(test_dir, 'Rust (Stage 2)')\n",
    "rust3_test = os.path.join(test_dir, 'Rust (Stage 3)')\n",
    "rust4_test = os.path.join(test_dir, 'Rust (Stage 4)')\n",
    "norust_test = os.path.join(test_dir, 'No Rust')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##runs a single time\n",
    "# os.mkdir(store_directory)\n",
    "# os.mkdir(train_dir)\n",
    "# os.mkdir(validation_dir)\n",
    "# os.mkdir(test_dir)\n",
    "# os.mkdir(rust_train)\n",
    "os.mkdir(norust_train)\n",
    "# os.mkdir(rust_validation)\n",
    "os.mkdir(norust_validation)\n",
    "# os.mkdir(rust_test)\n",
    "os.mkdir(norust_test)\n",
    "# os.mkdir(rust1_train)\n",
    "# os.mkdir(rust2_train)\n",
    "# os.mkdir(rust3_train)\n",
    "# os.mkdir(rust4_train)\n",
    "\n",
    "# os.mkdir(rust1_validation)\n",
    "# os.mkdir(rust2_validation)\n",
    "# os.mkdir(rust3_validation)\n",
    "# os.mkdir(rust4_validation)\n",
    "\n",
    "# os.mkdir(rust1_test)\n",
    "# os.mkdir(rust2_test)\n",
    "# os.mkdir(rust3_test)\n",
    "# os.mkdir(rust4_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "fnames =  os.listdir(original_dataset_dir_norust)\n",
    "print(len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #copies first 75 images to train \n",
    "filenames =  os.listdir(original_dataset_dir_rust1)\n",
    "for i in range(0, 20):\n",
    "    src = os.path.join(original_dataset_dir_rust1, filenames[i])\n",
    "    dst = os.path.join(rust1_train, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# #copies first 5 images to Validation \n",
    "\n",
    "for i in range(19, 24):\n",
    "    src = os.path.join(original_dataset_dir_rust1, filenames[i])\n",
    "    dst = os.path.join(rust1_validation, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#copies first 5 images to train \n",
    "for i in range(24, 27):\n",
    "    src = os.path.join(original_dataset_dir_rust1, filenames[i])\n",
    "    dst = os.path.join(rust1_test, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "##Rust 2\n",
    "filenames =  os.listdir(original_dataset_dir_rust2)\n",
    "for i in range(0, 55):\n",
    "    src = os.path.join(original_dataset_dir_rust2, filenames[i])\n",
    "    dst = os.path.join(rust2_train, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# #copies first 5 images to Validation \n",
    "\n",
    "for i in range(56, 61):\n",
    "    src = os.path.join(original_dataset_dir_rust2, filenames[i])\n",
    "    dst = os.path.join(rust2_validation, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#copies first 5 images to train \n",
    "for i in range(61, 66):\n",
    "    src = os.path.join(original_dataset_dir_rust2, filenames[i])\n",
    "    dst = os.path.join(rust2_test, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "############################################\n",
    "#Rust 3\n",
    "filenames =  os.listdir(original_dataset_dir_rust3)\n",
    "for i in range(0, 34):\n",
    "    src = os.path.join(original_dataset_dir_rust3, filenames[i])\n",
    "    dst = os.path.join(rust3_train, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# #copies first 5 images to Validation \n",
    "\n",
    "for i in range(32, 37):\n",
    "    src = os.path.join(original_dataset_dir_rust3, filenames[i])\n",
    "    dst = os.path.join(rust3_validation, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#copies first 5 images to train \n",
    "for i in range(36, 41):\n",
    "    src = os.path.join(original_dataset_dir_rust3, filenames[i])\n",
    "    dst = os.path.join(rust3_test, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "#####################\n",
    "#Rust4\n",
    "filenames =  os.listdir(original_dataset_dir_rust4)\n",
    "for i in range(0, 14):\n",
    "    src = os.path.join(original_dataset_dir_rust4, filenames[i])\n",
    "    dst = os.path.join(rust4_train, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# #copies first 5 images to Validation \n",
    "\n",
    "for i in range(13, 17):\n",
    "    src = os.path.join(original_dataset_dir_rust4, filenames[i])\n",
    "    dst = os.path.join(rust4_validation, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#copies first 5 images to train \n",
    "for i in range(17, 20):\n",
    "    src = os.path.join(original_dataset_dir_rust4, filenames[i])\n",
    "    dst = os.path.join(rust4_test, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "#####\n",
    "#No Rust\n",
    "filenames =  os.listdir(original_dataset_dir_norust)\n",
    "for i in range(0, 67):\n",
    "    src = os.path.join(original_dataset_dir_norust, filenames[i])\n",
    "    dst = os.path.join(norust_train, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for i in range(64, 70):\n",
    "    src = os.path.join(original_dataset_dir_norust, filenames[i])\n",
    "    dst = os.path.join(norust_validation, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for i in range(70, 78):\n",
    "    src = os.path.join(original_dataset_dir_norust, filenames[i])\n",
    "    dst = os.path.join(norust_test, filenames[i])\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='swish'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg19 (Functional)           (None, 10, 10, 512)       20024384  \n_________________________________________________________________\nflatten_18 (Flatten)         (None, 51200)             0         \n_________________________________________________________________\ndense_55 (Dense)             (None, 256)               13107456  \n_________________________________________________________________\ndense_56 (Dense)             (None, 5)                 1285      \n=================================================================\nTotal params: 33,133,125\nTrainable params: 13,108,741\nNon-trainable params: 20,024,384\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "conv_base.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 196 images belonging to 5 classes.\nFound 24 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'ttv_images'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(326, 326),\n",
    "        batch_size=6,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(326, 326),\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/17\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff19c1ec700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff19c1ec700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff19c1ec700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff11cd1ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff11cd1ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff11cd1ff70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "10/10 - 58s - loss: 11.1049 - accuracy: 0.3500 - val_loss: 3.0982 - val_accuracy: 0.3750\n",
      "Epoch 2/17\n",
      "10/10 - 56s - loss: 3.2440 - accuracy: 0.3103 - val_loss: 2.9458 - val_accuracy: 0.3125\n",
      "Epoch 3/17\n",
      "10/10 - 71s - loss: 2.5475 - accuracy: 0.4833 - val_loss: 3.1305 - val_accuracy: 0.3750\n",
      "Epoch 4/17\n",
      "10/10 - 76s - loss: 2.4695 - accuracy: 0.5172 - val_loss: 1.5478 - val_accuracy: 0.5625\n",
      "Epoch 5/17\n",
      "10/10 - 72s - loss: 1.9797 - accuracy: 0.5862 - val_loss: 2.3770 - val_accuracy: 0.3125\n",
      "Epoch 6/17\n",
      "10/10 - 65s - loss: 1.6795 - accuracy: 0.4500 - val_loss: 2.8963 - val_accuracy: 0.3125\n",
      "Epoch 7/17\n",
      "10/10 - 66s - loss: 1.5537 - accuracy: 0.5167 - val_loss: 1.1076 - val_accuracy: 0.6250\n",
      "Epoch 8/17\n",
      "10/10 - 62s - loss: 1.3483 - accuracy: 0.5172 - val_loss: 1.9560 - val_accuracy: 0.3750\n",
      "Epoch 9/17\n",
      "10/10 - 63s - loss: 1.1585 - accuracy: 0.6500 - val_loss: 0.9654 - val_accuracy: 0.5625\n",
      "Epoch 10/17\n",
      "10/10 - 61s - loss: 1.2186 - accuracy: 0.5833 - val_loss: 1.7912 - val_accuracy: 0.5625\n",
      "Epoch 11/17\n",
      "10/10 - 64s - loss: 0.8161 - accuracy: 0.7167 - val_loss: 1.2834 - val_accuracy: 0.5625\n",
      "Epoch 12/17\n",
      "10/10 - 63s - loss: 1.1537 - accuracy: 0.6207 - val_loss: 1.1296 - val_accuracy: 0.5625\n",
      "Epoch 13/17\n",
      "10/10 - 67s - loss: 1.2196 - accuracy: 0.5833 - val_loss: 1.3486 - val_accuracy: 0.5000\n",
      "Epoch 14/17\n",
      "10/10 - 64s - loss: 1.0752 - accuracy: 0.5500 - val_loss: 1.3418 - val_accuracy: 0.4375\n",
      "Epoch 15/17\n",
      "10/10 - 66s - loss: 0.8167 - accuracy: 0.6034 - val_loss: 1.1988 - val_accuracy: 0.5000\n",
      "Epoch 16/17\n",
      "10/10 - 69s - loss: 0.8385 - accuracy: 0.6833 - val_loss: 1.0699 - val_accuracy: 0.5000\n",
      "Epoch 17/17\n",
      "10/10 - 70s - loss: 0.8970 - accuracy: 0.6333 - val_loss: 1.5478 - val_accuracy: 0.4375\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# tensorboard = keras.callbacks.TensorBoard(log_dir='/output/{}'.format(time()))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=2e-5),metrics=['acc'])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=10,epochs=17,validation_data=validation_generator,validation_steps=4,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 27 images belonging to 5 classes.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "test acc: 0.5925925970077515\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(350, 350),\n",
    "        batch_size=4,\n",
    "        shuffle=False, \n",
    "        class_mode='categorical')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=10)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 27 images belonging to 5 classes.\n",
      "[[10  0  0  0  0]\n",
      " [ 1  0  3  0  0]\n",
      " [ 2  0  1  2  0]\n",
      " [ 1  0  2  2  0]\n",
      " [ 0  0  0  3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      norust       0.71      1.00      0.83        10\n",
      "       rust1       0.00      0.00      0.00         4\n",
      "       rust2       0.17      0.20      0.18         5\n",
      "       rust3       0.29      0.40      0.33         5\n",
      "       rust4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.48        27\n",
      "   macro avg       0.23      0.32      0.27        27\n",
      "weighted avg       0.35      0.48      0.40        27\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 2 2 0 2 0 0 3 2 3 3 2 3 0 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "test_data_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(326, 326),\n",
    "        batch_size=3,\n",
    "        shuffle=False, # v imp : do not shuffle in case of test data, when measuring precision and recall\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_steps_per_epoch = np.math.ceil(test_data_generator.samples / test_data_generator.batch_size)\n",
    "\n",
    "predictions = model.predict_generator(test_data_generator, steps=test_steps_per_epoch)\n",
    "\n",
    "val_preds = np.argmax(predictions, axis=-1)\n",
    "# for i in range(predictions.shape[0]):\n",
    "#     if predictions[i]>0.5:\n",
    "#         val_preds[i] = 1\n",
    "#     else:\n",
    "#         val_preds[i] = 0\n",
    "val_trues = test_data_generator.classes\n",
    "\n",
    "labels = test_data_generator.class_indices.keys()\n",
    "cm = confusion_matrix(val_trues, val_preds)\n",
    "print(cm)\n",
    "report = classification_report(val_trues, val_preds, target_names=labels)\n",
    "print(report) \n",
    "print(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7ff1055c41f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('signature_function', 'signature_key'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7ff1055c41f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('signature_function', 'signature_key'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7ff1055c41f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('signature_function', 'signature_key'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpvmjuhy_m/assets\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpvmjuhy_m/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}